[33;21m2024-04-28 18:15:55,420 - butterfly_op.py[line:23] - WARNING: Import universal_cuda fail[0m
[33;21m2024-04-28 18:15:55,422 - butterfly_op.py[line:27] - WARNING: Import hadamard_cuda fail[0m
[33;21m2024-04-28 18:15:55,429 - matrix_parametrization.py[line:23] - WARNING: Cannot import matrix_parametrization_cuda. Decomposers can only work on CPU mode[0m
[38;21m2024-04-28 18:15:55,550 - sparse_train.py[line:245] - INFO: dataset:
  name: fmnist
  root: /home/dataset/fashion-mnist
  train_valid_split_ratio: [0.9, 0.1]
  train_valid_split_seed: 1
  resize_mode: bicubic
  center_crop: 28
  n_test_samples: None
  n_valid_samples: None
  num_workers: 2
  img_height: 28
  img_width: 28
  in_channels: 1
  num_classes: 10
  transform: augmented
  shuffle: 1
  augment: None
criterion:
  name: ce
aux_criterion: None
optimizer:
  name: adam
  lr: 0.002
  weight_decay: 0.0001
  grad_clip_value: 1
scheduler:
  name: cosine
  lr_gamma: 0.99
  lr_min: 2e-05
run:
  experiment: fmnist_cnn_train
  n_epochs: 50
  batch_size: 32
  use_cuda: 1
  gpu_id: 0
  deterministic: 1
  random_state: 42
  log_interval: 200
  train_noise: 0
  grad_clip: False
  max_grad_value: 1
  do_distill: False
  compile: False
quantize:
  weight_bit: 8
  input_bit: 8
noise:
  phase_bias: 0
  phase_noise_std: 0
  gamma_noise_std: 0
  crosstalk_factor: 0.05
  random_state: 42
  weight_noise_std: 0.0
  crosstalk_flag: 0
  noise_flag: 0
  crosstalk_scheduler:
    interv_h: 24
    interv_v: 120
    interv_s: 13
checkpoint:
  save_best_model_k: 3
  checkpoint_dir: fmnist/cnn/train
  model_comment: row-only-without-opt_lr-0.0020_wb-8_ib-6_dm-magnitude_gm-gradient_im-uniform_cb-[1,1,16,16]_density-0.6_ls-13_lh-24_run-4
  resume: 0
  restore_checkpoint: 
  no_linear: 0
model:
  name: TeMPO_CNN
  conv_cfg:
    type: TeMPOBlockConv2d
    mode: weight
    miniblock: [1, 1, 16, 16]
    w_bit: 8
    in_bit: 6
  linear_cfg:
    type: TeMPOBlockLinear
    mode: weight
    miniblock: [1, 1, 16, 16]
    w_bit: 8
    in_bit: 6
  norm_cfg:
    type: BN2d
    affine: True
  act_cfg:
    type: ReLU
    inplace: True
  kernel_list: [64, 64, 64]
  kernel_size_list: [3, 3, 3]
  hidden_list: []
  stride_list: [1, 2, 1]
  padding_list: [1, 1, 1]
  pool_out_size: 5
  dst_scheduler:
    death_mode: magnitude
    growth_mode: gradient
    init_mode: uniform
    density: 0.6
debug:
  verbose: 1
  verboise: 1
dst_scheduler:
  death_rate: 0.5
  growth_death_ratio: 1.0
  death_rate_decay: cosine
  death_mode: magnitude
  growth_mode: gradient
  redistribution_mode: None
  spe_initial: None
  pruning_type: structure_row
  update_frequency: 1400
  T_max: 0.8
  density: 0.6
  max_combinations: 100
  init_mode: uniform
  power_choice_margin: 2
  splitter_biases: 90
  node_spacing: [20, 100][0m
[38;21m2024-04-28 18:16:08,604 - lsq.py[line:119] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int6 quantization with offset: True[0m
[38;21m2024-04-28 18:16:08,639 - utils.py[line:1139] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-04-28 18:16:14,612 - lsq.py[line:119] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int6 quantization with offset: True[0m
[38;21m2024-04-28 18:16:14,618 - utils.py[line:1139] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-04-28 18:16:14,629 - lsq.py[line:119] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int6 quantization with offset: True[0m
[38;21m2024-04-28 18:16:14,640 - utils.py[line:1139] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-04-28 18:16:14,652 - lsq.py[line:119] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int6 quantization with offset: True[0m
[38;21m2024-04-28 18:16:14,657 - utils.py[line:1139] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-04-28 18:16:14,681 - sparse_train.py[line:281] - INFO: TeMPO_CNN(
  (features): Sequential(
    (conv1): ConvBlock(
      (conv): TeMPOBlockConv2d(
        1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), miniblock=[1, 1, 16, 16], mode=weight
        (input_quantizer): ActQuantizer_LSQ({'nbits': 6, 'mode': 'tensor_wise', 'signed': True, 'offset': True})
        (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
        (phase_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      )
      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation): ReLU(inplace=True)
    )
    (conv2): ConvBlock(
      (conv): TeMPOBlockConv2d(
        64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), miniblock=[1, 1, 16, 16], mode=weight
        (input_quantizer): ActQuantizer_LSQ({'nbits': 6, 'mode': 'tensor_wise', 'signed': True, 'offset': True})
        (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
        (phase_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      )
      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation): ReLU(inplace=True)
    )
    (conv3): ConvBlock(
      (conv): TeMPOBlockConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), miniblock=[1, 1, 16, 16], mode=weight
        (input_quantizer): ActQuantizer_LSQ({'nbits': 6, 'mode': 'tensor_wise', 'signed': True, 'offset': True})
        (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
        (phase_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      )
      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation): ReLU(inplace=True)
    )
  )
  (pool2d): AdaptiveAvgPool2d(output_size=5)
  (classifier): Sequential(
    (fc1): LinearBlock(
      (linear): TeMPOBlockLinear(
        1600, 10, miniblock=[1, 1, 16, 16], mode=weight
        (input_quantizer): ActQuantizer_LSQ({'nbits': 6, 'mode': 'tensor_wise', 'signed': True, 'offset': True})
        (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
        (phase_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      )
    )
  )
)[0m
[38;21m2024-04-28 18:16:14,682 - dst2.py[line:277] - INFO: structure_row does not support power optimization, growth_opts reduced to [][0m
[38;21m2024-04-28 18:16:14,682 - dst2.py[line:310] - INFO: structure_row does not support power optimization, death_mode reduced to [][0m
[38;21m2024-04-28 18:16:14,683 - dst2.py[line:397] - INFO: created pruning mask.[0m
[38;21m2024-04-28 18:16:14,683 - dst2.py[line:1127] - INFO: structure_row not support power, init mode reduced to ['uniform'][0m
[38;21m2024-04-28 18:16:14,684 - dst2.py[line:1152] - INFO: Total Model parameters: 74752[0m
[38;21m2024-04-28 18:16:14,686 - dst2.py[line:1156] - INFO: Total parameters under density level of 0.6: 0.6023116438356164[0m
[38;21m2024-04-28 18:16:14,691 - dst2.py[line:1163] - INFO: Scale up initialized weights by (weight_count/nonzeros) to maintain the same variance[0m
[38;21m2024-04-28 18:16:14,691 - dst2.py[line:1172] - INFO: Zero counts:
	{'features.conv1.conv_0': 496, 'features.conv2.conv_1': 14448, 'features.conv3.conv_2': 14784}[0m
[38;21m2024-04-28 18:16:14,692 - dst2.py[line:1173] - INFO: Nonzero counts:
	{'features.conv1.conv_0': 528, 'features.conv2.conv_1': 22416, 'features.conv3.conv_2': 22080}[0m
[38;21m2024-04-28 18:16:14,692 - dst2.py[line:1174] - INFO: Param counts:
	{'features.conv1.conv_0': 1024, 'features.conv2.conv_1': 36864, 'features.conv3.conv_2': 36864}[0m
[38;21m2024-04-28 18:16:14,692 - dst2.py[line:399] - INFO: initialized pruning mask.[0m
[38;21m2024-04-28 18:16:14,692 - sparse_train.py[line:328] - INFO: Number of parameters: 100954[0m
[38;21m2024-04-28 18:16:14,692 - sparse_train.py[line:333] - INFO: Current checkpoint: ./checkpoint/fmnist/cnn/train/TeMPO_CNN_row-only-without-opt_lr-0.0020_wb-8_ib-6_dm-magnitude_gm-gradient_im-uniform_cb-[1,1,16,16]_density-0.6_ls-13_lh-24_run-4.pt[0m
[38;21m2024-04-28 18:16:14,900 - sparse_train.py[line:356] - INFO: Experiment fmnist_cnn_train (2) starts. Run ID: (291c570fac544be3a100078d1f06184b). PID: (97526). PPID: (97519). Host: (en4226598rl.eng.asu.edu)[0m
{}
/home/jiaqigu/pkgs/miniforge3/envs/python310venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[38;21m2024-04-28 18:16:16,167 - sparse_train.py[line:124] - INFO: Train Epoch: 1 [     32/  54000 (  0%)] Loss: 2.3248e+00 class Loss: 2.3248e+00[0m
/bin/sh: line 1: 97526 Terminated              python3 sparse_train.py configs/fmnist/cnn/train/sparse_train.yml --optimizer.lr=0.002 --run.random_state=42 --model.conv_cfg.w_bit=8 --model.linear_cfg.w_bit=8 --model.dst_scheduler.death_mode=magnitude --model.dst_scheduler.growth_mode=gradient --model.dst_scheduler.init_mode=uniform --model.dst_scheduler.density=0.6 --noise.crosstalk_scheduler.interv_s=13 --noise.crosstalk_scheduler.interv_h=24 --noise.noise_flag=0 --noise.crosstalk_flag=0 --model.conv_cfg.miniblock=[1,1,16,16] --model.linear_cfg.miniblock=[1,1,16,16] --model.conv_cfg.in_bit=6 --model.linear_cfg.in_bit=6 --checkpoint.model_comment=row-only-without-opt_lr-0.0020_wb-8_ib-6_dm-magnitude_gm-gradient_im-uniform_cb-[1,1,16,16]_density-0.6_ls-13_lh-24_run-4
